# ANN Model for Hybrid Nanofluid Boundary Layer Flow

## Overview

This project implements a **traditional Artificial Neural Network (ANN)** trained with the **Levenberg-Marquardt algorithm** to predict the flow and thermal characteristics of hybrid nanofluids over a stretching sheet. The model is trained on ground-truth numerical data generated by solving the coupled nonlinear ODEs from the manuscript.

**Key Features:**
- ✅ Pure data-driven ANN (not PINN)
- ✅ 9 hidden layers × 30 neurons with tanh activation
- ✅ Levenberg-Marquardt optimization
- ✅ Numerical ODE solver using scipy.integrate.solve_bvp
- ✅ Comprehensive dataset generation across parameter space
- ✅ Manuscript-style visualization

---

## Project Structure

```
ann-hybrid-flow/
├── data/                          # Generated datasets
│   ├── training_data.csv         # Training dataset
│   └── test_data.csv             # Test dataset
├── models/                        # Model implementations
│   ├── ann.py                    # ANN architecture
│   ├── lm_optimizer.py           # Levenberg-Marquardt optimizer
│   └── checkpoints/              # Saved models and scalers
├── solver/                        # Numerical ODE solver
│   └── ode_solver.py             # BVP solver for ground truth
├── plots/                         # Generated figures
├── generate_data.py              # Dataset generation script
├── train_ann.py                  # Training pipeline
├── plot_results.py               # Visualization script
├── requirements.txt              # Python dependencies
└── README.md                     # This file
```

---

## Physical Model

### Governing Equations

The model solves the following coupled ODEs from the manuscript:

**Momentum Equation (Eq. 8):**
```
(ν_hnf/ν_f) f''' + f·f'' + (2n)/(n+1)·(1 - f'^2) - 2/(n+1)·(σ_hnf/σ_f)·(ρ_f/ρ_hnf)·M·(f' - 1) = 0
```

**Energy Equation (Eq. 9):**
```
(κ_hnf/κ_f + Nr/(1 + (Tr-1)θ)^3)·θ'' + Pr·As·(f·θ' - 2(2n-1)/(n+1)·f'·θ) 
  + 3·Nr·(Tr-1)/(1 + (Tr-1)θ)^2·(θ')^2 = 0
```

**Boundary Conditions (Eq. 10):**
```
At η = 0:
  f(0) = 0
  f'(0) = 1 + β·f''(0)
  θ'(0) = -Nh·(1 - θ(0))

At η → ∞:
  f'(∞) → 1
  θ(∞) → 0
```

### Parameters

| Symbol | Description | Range |
|--------|-------------|-------|
| M | Magnetic parameter | [0.5, 1.0, 2.0] |
| Nr | Radiation parameter | [0.2, 0.5, 1.0] |
| Nh | Convective heat transfer parameter | [0.2, 0.5, 1.0] |
| λ | Stretching parameter | [0.5, 1.0, 2.0] |
| β | Velocity slip parameter | 0.1 |
| Pr | Prandtl number | 6.2 (water-based) |
| n | Power-law index | 1.0 |
| Tr | Temperature ratio | 1.5 |
| As | Unsteadiness parameter | 1.0 |

### Engineering Quantities

- **Skin Friction Coefficient:** `Cf = f''(0)`
- **Nusselt Number:** `Nu = -θ'(0)`

---

## Installation

### Prerequisites
- Python 3.10 or higher
- pip package manager

### Setup

1. **Clone or navigate to the project directory:**
```bash
cd "e:/University/FYDP/ANN net"
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

Required packages:
- torch >= 2.0.0
- numpy >= 1.24.0
- scipy >= 1.10.0
- matplotlib >= 3.7.0
- pandas >= 2.0.0
- tqdm >= 4.65.0
- scikit-learn >= 1.3.0

---

## Usage

### Step 1: Generate Training Data

Generate ground-truth numerical solutions across the parameter space:

```bash
python generate_data.py
```

**What it does:**
- Solves the ODE system for 81 parameter combinations (3×3×3×3)
- Generates 400 points per case (η ∈ [0, 10])
- Saves to `data/training_data.csv` (~32,400 samples)
- Also generates 10 random test cases

**Output:**
```
data/training_data.csv  # Main dataset
data/test_data.csv      # Additional test cases
```

**Expected runtime:** ~5-10 minutes

---

### Step 2: Train the ANN Model

Train the ANN using Levenberg-Marquardt optimization:

```bash
python train_ann.py
```

**What it does:**
- Loads and normalizes the dataset
- Splits data: 80% train, 10% validation, 10% test
- Trains 9-layer ANN (30 neurons/layer) with tanh activation
- Uses Levenberg-Marquardt optimizer (scipy implementation)
- Implements early stopping (patience = 20 epochs)
- Saves best model and scalers

**Output:**
```
models/checkpoints/best_model.pth       # Trained model
models/checkpoints/scaler_*.pkl         # Normalization scalers
plots/training_history.png              # Loss curves
```

**Expected runtime:** ~10-30 minutes (depends on convergence)

**Training Configuration:**
- Epochs: 100 (with early stopping)
- Batch size: 2000
- Optimizer: Levenberg-Marquardt (scipy)
- Loss: MSE on [f, θ]

---

### Step 3: Generate Plots

Create manuscript-style visualizations:

```bash
python plot_results.py
```

**What it does:**
- Loads trained model
- Generates velocity and temperature profiles
- Compares ANN predictions with numerical solutions
- Plots skin friction and Nusselt number variations

**Output:**
```
plots/velocity_profile_M.png        # f'(η) for varying M
plots/temperature_profile_Nr.png    # θ(η) for varying Nr
plots/ann_vs_numerical.png          # ANN vs numerical comparison
plots/cf_vs_M.png                   # Skin friction vs M
plots/nu_vs_Nr.png                  # Nusselt number vs Nr
```

---

## Model Architecture

### ANN Structure

```
Input Layer:     1 neuron  (η)
                 ↓
Hidden Layer 1:  30 neurons (tanh)
Hidden Layer 2:  30 neurons (tanh)
Hidden Layer 3:  30 neurons (tanh)
Hidden Layer 4:  30 neurons (tanh)
Hidden Layer 5:  30 neurons (tanh)
Hidden Layer 6:  30 neurons (tanh)
Hidden Layer 7:  30 neurons (tanh)
Hidden Layer 8:  30 neurons (tanh)
Hidden Layer 9:  30 neurons (tanh)
                 ↓
Output Layer:    2 neurons  (f, θ)
```

**Total Parameters:** ~8,732

**Initialization:** Xavier uniform

**Activation:** Hyperbolic tangent (tanh)

---

## Levenberg-Marquardt Optimization

The LM algorithm combines:
- **Gradient Descent** (for far from optimum)
- **Gauss-Newton** (for near optimum)

**Update Rule:**
```
(J^T·J + λ·I)·Δp = -J^T·r

where:
  J = Jacobian matrix
  r = residuals (predictions - targets)
  λ = damping parameter (adaptive)
  Δp = parameter update
```

**Adaptive Damping:**
- Successful step → λ = λ × 0.1 (move toward Gauss-Newton)
- Failed step → λ = λ × 10 (move toward gradient descent)

**Implementation:**
- Primary: `scipy.optimize.least_squares` (stable, efficient)
- Alternative: Custom PyTorch implementation (for research)

---

## Results & Validation

### Expected Performance

**Test Set Metrics:**
- MSE (f): < 1e-4
- MSE (θ): < 1e-4
- MAE (overall): < 1e-3
- Max Error: < 1e-2

### Validation Checks

1. **Boundary Conditions:**
   - f(0) ≈ 0
   - f'(0) ≈ 1 + β·f''(0)
   - f'(∞) ≈ 1
   - θ(∞) ≈ 0

2. **Physical Consistency:**
   - Monotonic velocity profiles
   - Decaying temperature profiles
   - Positive skin friction
   - Positive Nusselt number

3. **Parameter Trends:**
   - Increasing M → decreased velocity
   - Increasing Nr → enhanced heat transfer
   - Increasing Nh → modified thermal boundary layer

---

## Troubleshooting

### Common Issues

**1. ODE Solver Fails:**
```
Solution: Adjust initial guess or increase max_nodes
File: solver/ode_solver.py, line 120
```

**2. Training Loss Not Decreasing:**
```
Solution: 
- Check data normalization
- Reduce learning rate (adjust λ_init in LM optimizer)
- Increase max_nfev in scipy optimizer
```

**3. Memory Error:**
```
Solution: Reduce batch_size in train_ann.py
Current: 2000 → Try: 1000 or 500
```

**4. Plots Not Generated:**
```
Solution: Ensure model is trained first
Run: python train_ann.py before python plot_results.py
```

---

## File Descriptions

### Core Scripts

**`generate_data.py`**
- Generates numerical solutions using scipy.integrate.solve_bvp
- Creates training and test datasets
- Handles parameter grid generation

**`train_ann.py`**
- Implements training pipeline
- Data loading and preprocessing
- Model training with LM optimizer
- Evaluation and checkpointing

**`plot_results.py`**
- Visualization module
- Manuscript-style figure generation
- ANN vs numerical comparison

### Model Files

**`models/ann.py`**
- ANN architecture definition
- Forward pass implementation
- Automatic differentiation support

**`models/lm_optimizer.py`**
- Custom LM optimizer (PyTorch)
- Scipy wrapper for LM
- Jacobian computation

**`solver/ode_solver.py`**
- Numerical ODE solver
- Boundary condition implementation
- Engineering quantity computation

---

## Extending the Project

### Add New Parameters

1. Modify `generate_data.py`:
```python
new_param_values = [value1, value2, value3]
# Add to parameter grid generation
```

2. Update solver in `solver/ode_solver.py`:
```python
self.new_param = params.get('new_param', default_value)
# Include in ODE equations
```

### Change ANN Architecture

Edit `models/ann.py`:
```python
model = HybridNanofluidANN(
    input_dim=1,
    hidden_dim=50,        # Change neurons per layer
    num_hidden_layers=12, # Change number of layers
    output_dim=2
)
```

### Use Different Optimizer

In `train_ann.py`:
```python
# Option 1: Custom LM
trainer = Trainer(model, optimizer_type='lm_custom')

# Option 2: Scipy LM (recommended)
trainer = Trainer(model, optimizer_type='lm_scipy')
```

---

## Citation

If you use this code, please cite the manuscript:

```
[Manuscript citation to be added]
```

---

## License

This project is for academic and research purposes.

---

## Contact

For questions or issues:
- Check the troubleshooting section
- Review the manuscript for physical model details
- Verify all dependencies are installed correctly

---

## Acknowledgments

- Numerical solver: scipy.integrate.solve_bvp
- Deep learning framework: PyTorch
- Optimization: Levenberg-Marquardt algorithm
- Manuscript: [Reference to be added]

---

**Last Updated:** 2025-11-21
**Version:** 1.0.0
**Status:** ✅ Complete and tested
